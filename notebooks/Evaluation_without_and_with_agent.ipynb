{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0516df6a-56fa-41f1-9fc0-69c0b5962f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06615d7-3055-4466-9c50-8e40e6675dac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -r requirements.txt\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b527d5-3fe9-473b-af6c-ee363be12855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import All Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719c2ffc-b493-4ca6-b5d8-33ccfa2fee45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
    "# Import necessary libraries for PySpark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql import SparkSession, Row\n",
    "############\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import datetime\n",
    "import ast\n",
    "import time\n",
    "##################################\n",
    "# from utility import MultiModel, Processing\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import autogen\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "# vision_model = MultiModel(api_key=\"9a66c4f7a6304b9ab03c4836c29cf79a\", temp=0)\n",
    "# process = Processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc445de-1136-472c-859c-5ad35e0e2c94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Azure Storage Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea16e61-7eb4-4f00-87f8-60ac3bd68468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_access_key}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f9016b7-5156-4e38-a34b-7da0b1476bff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Without Autogen Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9492d99e-39fe-4c0f-bdb5-3dbbc7dfde60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MultiModel():\n",
    "    \"\"\"\n",
    "    A class to interact with the GPT-4 Vision model.\n",
    "\n",
    "    Attributes:\n",
    "        api_key (str): The API key for authentication.\n",
    "        temp (float): The temperature parameter for the model. Default is 0.\n",
    "    \"\"\"\n",
    "    def __init__(self,api_key,temp=0):\n",
    "        \"\"\"\n",
    "        The constructor for the MultiModel class.\n",
    "        \"\"\"\n",
    "        self.api_key=api_key\n",
    "        self.temp=temp\n",
    "        \n",
    "    def gpt4v_img(self,prompt,image_path,max_tokens:int=4000):\n",
    "        \"\"\"\n",
    "        Method to generate a response from the GPT-4 Vision model.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str): The prompt for the model.\n",
    "            image_path (str): The path to the image file.\n",
    "            max_tokens (int): The maximum number of tokens for the model to generate. Default is 4000.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the model's response and inference time.\n",
    "        \n",
    "        \"\"\"\n",
    "        def encode_image(image_path):\n",
    "            \"\"\"\n",
    "            Function to encode an image file into base64 format.\n",
    "\n",
    "            Parameters:\n",
    "                image_path (str): The path to the image file.\n",
    "\n",
    "            Returns:\n",
    "                str: The base64 encoded string of the image.\n",
    "            \"\"\"\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "        # Getting the base64 string\n",
    "        base64_image = encode_image(image_path)\n",
    "        headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"api-key\": self.api_key,\n",
    "            }\n",
    "\n",
    "        payload = {\n",
    "              \"model\": \"gpt-4-vision-preview\",\n",
    "              \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"{prompt}\"},\n",
    "                                                 {\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\",\"detail\": \"high\"}}]\n",
    "                    }],\n",
    "              \"max_tokens\": max_tokens,\n",
    "            \"temperature\":self.temp\n",
    "                }\n",
    "        \n",
    "        start=time.time()\n",
    "        response = requests.post(\"https://chatgpt4o.openai.azure.com/openai/deployments/gpt4odeployment/chat/completions?api-version=2023-03-15-preview\", headers=headers, json=payload)\n",
    "        end=time.time()\n",
    "        ## Calculate the inferance time\n",
    "        latency=end-start\n",
    "        ## Lets Find only Final model response\n",
    "        print(response.json())\n",
    "        result = response.json()['choices'][0]['message']['content']\n",
    "        token=response.json()[\"usage\"]['total_tokens']\n",
    "        return {\"final_response\":result,\"inferance_time\":latency,\"token\":token}\n",
    "    \n",
    "class Processing():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def save_uploadedfile(self,uploadedfile):\n",
    "        \"\"\"\n",
    "        Function to save an uploaded file.\n",
    "\n",
    "        Parameters:\n",
    "            uploadedfile: The file uploaded by the user.\n",
    "\n",
    "        Returns:\n",
    "            str: The path where the file is saved.\n",
    "        \"\"\"\n",
    "\n",
    "        # Join the directory \"images\" and the name of the uploaded file to form the path where the file will be saved\n",
    "        saved_file_path = os.path.join(\"images\", uploadedfile.name)\n",
    "\n",
    "        # Open the file in write mode\n",
    "        with open(saved_file_path, \"wb\") as f:\n",
    "            # Write the contents of the uploaded file to the new file\n",
    "            f.write(uploadedfile.getbuffer())\n",
    "            # Display a success message in the Streamlit app\n",
    "            st.success(\"Saved File: {} to tempDir\".format(uploadedfile.name))\n",
    "\n",
    "        # Return the path where the file is saved\n",
    "        return saved_file_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043f816d-dee9-40a8-8a60-952b67e1ac12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pre-defined Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc1adb5-7239-4105-991e-ce856f4002ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tags = \"\"\"products category:TOPWEAR,BOTTOMWEAR\n",
    "TOPWEAR--\n",
    "\n",
    "Products:Shirt,Jumpsuit,Dungarees,Sleepwear,Sweatshirt,Innerwear,Dress,Top,T-shirt,Polo Collar T-shirt,Sweater/Cardigan,Hoodie,Jackets,Raincoat,LongCoat,Blazer,Suits (2-piece or 3-piece Men),Kurta,Kurta Set,Maxi Dress,Showerproof,vest,Sports Bra, Polo shirt.\n",
    "Color:Pink,Red,Blue,Black,Grey,Navy Blue,Charcoal Grey,White,Green,Olive,Brown,Beige,Khaki,Cream,Maroon,Off White,Grey Melange,Teal,Coffee Brown,Mustard,Purple,Rust,Sea Green,Burgundy,Turquoise Blue,Taupe,Silver,Mauve,Orange,Yellow,Multi,Lavender,Tan,Peach,Magenta,Fluorescent Green,Coral,Copper.\n",
    "Gender:Men,Women,Girls,Boys.\n",
    "Pattern:Striped,Checked,Embellished,Ribbed,Colorblocked,Dyed,Printed,Embroidered,Self-Design,Solid,Graphic,Floral,Polka Dots,Camouflage,Animal,Self-Design,Ombre.\n",
    "Silhouette:A-line,Peplum,Balloon,Fit and Flare,Sheath,Bodycon,Shirt Style,Jumper,Wrap,Kaftan.\n",
    "Neckline:Turtle/High Neck,Round Neck,Square Neck,Halter Neck,Scoop Neck,V neck Boat Neck,Polo Collar,Open-Collar,Crew Neck,Tie-Up Neck,Boat Neck,Shirt Neck, pointed collar neckline, mandarin collar,notch lapel.\n",
    "Sleeve Length:Sleeveless,Half Sleeves,Long Sleeves,Three-Quarter Sleeves, short sleeve,cap sleeve.\n",
    "Sleeve Style:Batwing Sleeves,Bell Sleeves,Flared Sleeves,Balloon Sleeves,Puffed Sleeves,Cold Sleeves,Shoulder Sleeves,Regular Sleeves,Slit Sleeves,Roll Up Sleeves,No Sleeves,Flutter Sleeve.\n",
    "Fabric:Cotton,Polyester,Leather,Denim,Silk,Wool,Corduroy,Fleece,Schiffli,Terry,Crepe,Net,Georgette.\n",
    "Brand:Mango,Puma,Adidas,Nike,Calvin Klein,Lacoste,Fred Perry,Brooks Brothers,GAP,Levis,GANT,Superdry,Tommy Hilfiger,H&M,Zara,Louis Phillipe,Polo Raulph Lauren,Guess,Gucci,Prada,Versace,Aeropostale,Abercrombie & Fitch,DKNY,Michael Kors,Coach,Fendi.\n",
    "Occasion:Casual,Formal,Parties/Evening,Sports,Maternity,Ethnic,Everyday,Work,winters.\n",
    "Fit Type:Slim Fit,Oversized,Regular,Skinny Fit,Loose Fit.\n",
    "Top Wear Length:Midi,Maxi,Mini,Above Knee,Cropped,Regular,hip length, waist length.\n",
    "\n",
    "BOTTOMWEAR--\n",
    "\n",
    "Products:Trouser,Jeans,Shorts,Trackpants,Joggers,Cargos,Skirts, dress pants,leggings, pants.\n",
    "Color:Pink,Red,Blue,Black,Grey,Navy Blue,Charcoal Grey,White,Green,Olive,Brown,Beige,Khaki,Cream,Maroon,Off White,Grey Melange,Teal,Coffee Brown,Pink,Mustard,Purple,Rust,Sea Green,Burgundy,Turquoise Blue,Taupe,Silver,Mauve,Orange,Yellow,Multi,Lavender,Tan,Peach,Magenta,Fluorescent Green,Coral,Copper.\n",
    "Gender:Men,Women,Girls,Boys.\n",
    "Pattern:Striped,Checked,Embellished,Ribbed,Colorblocked,Dyed,Printed,Embroidered,Self-Design,Solid,Graphic,Floral,Polka Dots,Camouflage,Animal,Self-Design,Ombre.\n",
    "Bottom Style:Flared,Straight,Loose Fit,A-Line,Peplum,Skinny Fit,Pencil, Slim,wide leg,tapered,wide leg.\n",
    "Bottom Length:Above Knee,Below Knee,Knee Length,Midi,Mini,Ankle,Maxi,Regular length,full length.\n",
    "Waist Rise:High-Rise,Low-Rise,Mid-Rise.\n",
    "Fabric:Cotton,Chambray,Polyester,Leather,Denim,Corduroy,Silk,Wool,Fleece,Velvet.\n",
    "Brand:Mango,Puma,Adidas,Nike,Calvin Klein,Lacoste,Fred Perry,Brooks Brothers,GAP,Levis,GANT,Superdry,Tommy Hilfiger,H&M,Zara,Louis Phillipe,Polo Raulph Lauren,Guess,Gucci,Prada,Versace,Aeropostale,Abercrombie & Fitch,DKNY,Michael Kors,Coach,Fendi.\n",
    "Occasion:Casual,Formal,Parties/Evening,Sports,Maternity,Ethnic,Everyday,Work.\n",
    "Fit Type:Slim Fit,Oversized,Regular,Skinny Fit,Loose Fit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3db50d-2359-4890-b908-60a63d16391a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Azure Open Ai model - GPT-4O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a840a037-00e2-45ea-93af-1e1b6cc238b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vision_model = MultiModel(api_key=dbutils.secrets.get(scope=\"dbx-us-scope\",key=\"azure-openai-llm-api-key\") , temp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "331e2d74-a6e8-43e3-ace0-82ddbc87dbff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffdfa68-926e-4a6e-a8ad-bf6c1f5bf971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"adlsusdldev02\"\n",
    "storage_account_access_key ==dbutils.secrets.get(scope=\"dbx-us-scope\",key=\"storage-account-access-key\") \n",
    "container_name = \"intellitag\"\n",
    "mount_point = \"/mnt/my_intellitag_mount\"\n",
    "database_name = \"intellitag_catalog.intellitag_dbx\"\n",
    "table_name = \"Result_Logs\"\n",
    "file_metadata=\"File_Metadata\"\n",
    "\n",
    "model=\"gpt4odeployment\"\n",
    "api_key= dbutils.secrets.get(scope=\"dbx-us-scope\",key=\"azure-openai-llm-api-key\") \n",
    "base_url= dbutils.secrets.get(scope=\"dbx-us-scope\",key=\"azure-openai-llm-base-url\") \n",
    "api_type= \"azure\"\n",
    "api_version= \"2024-02-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e725c4f-592c-49e8-ad4a-f00b0f6ff1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def list_jpg_files_recursively(directory):\n",
    "    \"\"\"\n",
    "    Recursively lists all .jpg files in a given directory and its subdirectories on DBFS, \n",
    "    and returns their paths as a list with paths formatted for local access.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path of the directory to search for .jpg files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths for all .jpg files found, with DBFS paths replaced by local file paths.\n",
    "    \"\"\"\n",
    "    files = []  # Initialize an empty list to store the .jpg file paths\n",
    "    \n",
    "    # List all items (files and directories) in the provided directory\n",
    "    items = dbutils.fs.ls(directory)\n",
    "    \n",
    "    for item in items:\n",
    "        if item.isDir():\n",
    "            # If the item is a directory, recursively search within it\n",
    "            files.extend(list_jpg_files_recursively(item.path))\n",
    "        elif item.path.endswith(\".jpg\"):\n",
    "            # If the item is a .jpg file, add its path to the list\n",
    "            # Convert DBFS path to a local path format (replace \"dbfs:/\" with \"/dbfs/\")\n",
    "            files.append(item.path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    \n",
    "    return files  # Return the list of .jpg file paths\n",
    "\n",
    "#+\"/Intellitag_Uploads\" #+ \"/Intellitag_Testing\"  #\"/Origamis Intellitag Testing\"\n",
    "\n",
    "directory_to_search = mount_point+ \"/Intellitag_Testing/\" \n",
    "\n",
    "# Get the list of all .jpg files\n",
    "jpg_files = list_jpg_files_recursively(directory_to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f766e93-d2e8-4933-a429-de24d5688dd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filter the Data\n",
    "Find the images which has the ground truth in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490ad225-6499-4ef9-aa5d-2ebfcea39683",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_wear_df=pd.read_excel(\"All-wears-groundtruths-json-top-bottom.xlsx\",sheet_name=\"top-wear-updated\").fillna(\"\")\n",
    "bottom_wear_df=pd.read_excel(\"All-wears-groundtruths-json-top-bottom.xlsx\",sheet_name=\"bottom-wear-updated\").fillna(\"\")\n",
    "\n",
    "top_wear_product_list=top_wear_df['KEY'].to_list()\n",
    "bottom_wear_product_list=bottom_wear_df['KEY'].to_list()\n",
    "\n",
    "product_list=set(top_wear_product_list+bottom_wear_product_list)\n",
    "\n",
    "final_filter_product_list=[img_path for img_path in product_list if \"/dbfs/mnt/my_intellitag_mount/Intellitag_Testing/\" + img_path in jpg_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bd5010-38c6-484a-8cf5-2f6751c36cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Without Agent Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ab541d-e407-4564-97f3-30a23f2df43e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model prompt\n",
    "# Given is the image of clothing catalog.Your task is to identify items or products of clothing only which is in focus from a given image and based on the identified items or products\n",
    "metadata_json = {\"generated_tags\": [\n",
    "                                {\n",
    "                                \"products category\": \"string\",\n",
    "                                \"Products\": \"string\",\n",
    "                                \"Color\": \"string\",\n",
    "                                \"Gender\": \"string\",\n",
    "                                \"Pattern\": \"string\",\n",
    "                                \"Silhouette\": \"string\",\n",
    "                                \"Neckline\": \"string\",\n",
    "                                \"Sleeve Length\": \"string\",\n",
    "                                \"Sleeve Style\": \"string\",\n",
    "                                \"Fabric\": \"string\",\n",
    "                                \"Brand\": \"string\",\n",
    "                                \"Occasion\": \"string\",\n",
    "                                \"Fit Type\": \"string\",\n",
    "                                \"Top Wear Length\": \"string\",\n",
    "                                }\n",
    "                                ]\n",
    "                }\n",
    "\n",
    "predictor_system_message = f\"\"\"\n",
    "You will receive an image_path as input.\n",
    "1. Analyze the provided catalog image and identify all the topwear and/or bottomwear items that are in focus and fully captured within the image.\n",
    "2. For each identified clothing item, extract the following metadata:\n",
    "\t{tags}\n",
    "3. Organize the metadata for each identified clothing item in a structured JSON format, with each item represented as a dictionary containing the specified keys.\n",
    "4. If the color recognition tags provided do not cover all the colors present in the image, feel free to use additional color descriptors as necessary.\n",
    "5. Ensure that the gender and material composition type are correctly mapped based on the identified products.\n",
    "6. Carefully match the provided tags with the appropriate metadata entities, and do not include any additional comments in the JSON output.\n",
    "7. If the image contains clothing items that are not fully captured or are out of focus, do not include them in the output.\n",
    "8. If the image does not contain any topwear or bottomwear items, or if the provided tags do not match the contents of the image, respond with an empty JSON array.\n",
    "<thinking>\n",
    "Review the provided image and tags, and extract the relevant metadata for each identified clothing item according to the instructions above. Organize the metadata in a structured JSON format, ensuring that all required fields are populated accurately.\n",
    "</thinking>\n",
    "\n",
    "Answer in the below format and DO NOT explain anything else:\n",
    "\n",
    "Format:\n",
    " {str(metadata_json)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bf2115-914a-4d2f-9638-cfa5bd241d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def json_sorting(predicted_output,groundtruth_output):\n",
    "    \"\"\"\n",
    "    Sorts predicted and ground truth JSON objects into two categories: 'top-wear' and 'bottom-wear'.\n",
    "    It appends the corresponding JSON objects based on the 'products category' key into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        predicted_output (list): A list of dictionaries representing predicted output, where each \n",
    "                                 dictionary contains a 'products category' key with values like \"TOPWEAR\" or \"BOTTOMWEAR\".\n",
    "        groundtruth_output (list): A list of dictionaries representing ground truth output, where each \n",
    "                                   dictionary contains a 'products category' key with values like \"TOPWEAR\" or \"BOTTOMWEAR\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys: 'top-wear' and 'bottom-wear'. Each key contains a list of dictionaries \n",
    "              (combined from predicted and ground truth) sorted into their respective categories.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store sorted items into 'top-wear' and 'bottom-wear' categories\n",
    "    category_dict={'top-wear':[],'bottom-wear':[]}\n",
    "\n",
    "    # Loop through each dictionary in the predicted output\n",
    "    for dict1 in predicted_output:\n",
    "        # Check if the 'products category' is 'TOPWEAR', and append to the 'top-wear' list\n",
    "        if dict1[\"products category\"]==\"TOPWEAR\":\n",
    "            category_dict[\"top-wear\"].append(dict1)\n",
    "        # Check if the 'products category' is 'BOTTOMWEAR', and append to the 'bottom-wear' list\n",
    "        elif dict1[\"products category\"]==\"BOTTOMWEAR\":\n",
    "            category_dict[\"bottom-wear\"].append(dict1)\n",
    "\n",
    "    # Loop through each dictionary in the ground truth output\n",
    "    for dict1 in groundtruth_output:\n",
    "        # Check if the 'products category' is 'TOPWEAR', and append to the 'top-wear' list\n",
    "        if dict1[\"products category\"]==\"TOPWEAR\":\n",
    "            category_dict[\"top-wear\"].append(dict1)\n",
    "        # Check if the 'products category' is 'BOTTOMWEAR', and append to the 'bottom-wear' list\n",
    "        elif dict1[\"products category\"]==\"BOTTOMWEAR\":\n",
    "            category_dict[\"bottom-wear\"].append(dict1)\n",
    "            \n",
    "    return category_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1c72f1-d2e0-4241-9bb8-a1b3e916f91f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_score(category_dict):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score based on predicted and ground truth values for different categories \n",
    "    such as 'top wear' and 'bottom wear'. The function compares the predicted values with the ground truth,\n",
    "    determines correctness, and computes a score for each category and an overall average score.\n",
    "\n",
    "    Args:\n",
    "        category_dict (dict): A dictionary where keys are category types (e.g., 'top wear', 'bottom wear'),\n",
    "                              and values are tuples containing predicted values and ground truth values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "              - 'avg_score': The average accuracy score across all categories.\n",
    "              - 'sum_corrected_values': Total number of incorrect predictions.\n",
    "              - 'sum_predicted_values': Total number of predicted values considered.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to store scores and counts\n",
    "    score_dict={}\n",
    "    predicted_values_dict={}\n",
    "    corrected_values_dict={}\n",
    "\n",
    "    # Loop through each category type in the category dictionary\n",
    "    for category_type in category_dict:\n",
    "        number_of_corrected_values = 0  # To count incorrect predictions\n",
    "        number_of_predicted_values=0    # To count total predictions\n",
    "\n",
    "        print(\"-------------------------------category key-------------------\")\n",
    "        print(category_type) # Print the current category, e.g., 'top wear', 'bottom wear'\n",
    "\n",
    "        # Retrieve the predicted values and ground truth for the category\n",
    "        final_predict,groundtruth=category_dict[category_type]\n",
    "        \n",
    "        # Loop through each key in the predicted values\n",
    "        for key in final_predict:\n",
    "            # Check if the predicted key exists in the ground truth and is not a category label\n",
    "            if key in groundtruth.keys() and key not in [\"products category\"]:\n",
    "\n",
    "                # Treat 'Not Available' in ground truth as equivalent to 'Not Visible'\n",
    "                if groundtruth[key]==\"Not Available\":\n",
    "                    groundtruth[key]=\"Not Visible\"\n",
    "\n",
    "                # Convert to lowercase and check if predicted and ground truth values match\n",
    "                if groundtruth[key].lower().replace(\"-\",\" \") in  final_predict[key].lower().replace(\"-\",\" \") or final_predict[key].lower().replace(\"-\",\" \") in groundtruth[key].lower().replace(\"-\",\" \"):\n",
    "                    # Step-1 :: Check if ground truth is present and predicted value is missing\n",
    "                    if groundtruth[key]!=\"\" and final_predict[key]==\"\":\n",
    "                        number_of_predicted_values+=1\n",
    "                        number_of_corrected_values+=1\n",
    "                        print(\"============================================\")\n",
    "                        print(f\"key :: {key} ::: Ground Truth :: {groundtruth[key]} ::: Predicted Value :: {final_predict[key]} ::: Status :: Incorrect\")\n",
    "                    else:\n",
    "                        number_of_predicted_values+=1\n",
    "                        print(\"============================================\")\n",
    "                        print(f\"key :: {key} ::: Ground Truth :: {groundtruth[key]} ::: Predicted Value :: {final_predict[key]} ::: Status :: Correct\")\n",
    "                else:\n",
    "                    # If prediction is incorrect, increment counts\n",
    "                    number_of_corrected_values+=1\n",
    "                    number_of_predicted_values+=1\n",
    "                    print(\"=============================================\")\n",
    "                    print(f\"key :: {key} ::: Ground Truth :: {groundtruth[key]} ::: Predicted Value :: {final_predict[key]} ::: Status :: Incorrect\")\n",
    "                   \n",
    "            else:\n",
    "                pass\n",
    "                # If predicted keys are not found in the ground truth, skip\n",
    "                # print(\"Predicted keys not found in the groundtruth ::\",key)\n",
    "\n",
    "        # print for summary of the current category's results        \n",
    "        print(\"-----------------------------------------------------\")\n",
    "        print(f\"Type :: {category_type}  :::  number_of_corrected_values :: {number_of_corrected_values} ::: number_of_predicted_values:{number_of_predicted_values}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        # Calculate score for the current category (accuracy)\n",
    "        score_dict[category_type]=1-(number_of_corrected_values/number_of_predicted_values)\n",
    "        corrected_values_dict[category_type]=number_of_corrected_values\n",
    "        predicted_values_dict[category_type]=number_of_predicted_values\n",
    "    \n",
    "    # Calculate the average score across all categories\n",
    "    avg_score=sum(score_dict.values())/2\n",
    "\n",
    "    # Sum the total corrected and predicted values across all categories\n",
    "    sum_corrected_values=sum(corrected_values_dict.values())\n",
    "    sum_predicted_values=sum(predicted_values_dict.values())\n",
    "    \n",
    "    # Return a dictionary containing the overall results\n",
    "    return {\"avg_score\":avg_score,\"sum_corrected_values\":sum_corrected_values,\"sum_predicted_values\":sum_predicted_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4213dcfa-66af-428c-bbed-910c1c1789f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_dict={}\n",
    "result_list=[]\n",
    "\n",
    "print(\"Total Number Test Images ::\",len(final_filter_product_list))\n",
    "\n",
    "# Loop through each file path in the list of final filtered products\n",
    "for file_path in final_filter_product_list:\n",
    "    #--------------------- Load and display the image----------------------------\n",
    "    # Construct the full image path by appending the file path to the DBFS mount location\n",
    "    image_path=\"/dbfs/mnt/my_intellitag_mount/Intellitag_Testing/\" + file_path\n",
    "\n",
    "    # Find the ground truth for the given image by filtering the top-wear and bottom-wear dataframes\n",
    "    top_wear_filter_list=top_wear_df.loc[top_wear_df['KEY']==file_path].to_dict(orient='records')\n",
    "    bottom_wear_filter_list=bottom_wear_df.loc[bottom_wear_df['KEY']==file_path].to_dict(orient='records')\n",
    "    # Combine the filtered lists from top-wear and bottom-wear to form the ground truth\n",
    "    groundtruth=top_wear_filter_list+bottom_wear_filter_list\n",
    "     \n",
    "    #----------------------- Run the Custom Flow --------------------------------\n",
    "    # Call the vision model's function to get predictions for the image\n",
    "    output = vision_model.gpt4v_img(\n",
    "        predictor_system_message, image_path=image_path)\n",
    "    \n",
    "    # converting the string response into a Python dictionary\n",
    "    predicted_dict = eval(output['final_response'].replace(\"```json\", '').replace('```', ''))\n",
    "\n",
    "    #----------------------Get final dict --------------------------\n",
    "    # Store the predicted results in the result_dict with the file_path as the key\n",
    "    result_dict[file_path]=predicted_dict\n",
    "   \n",
    "    # Get the final predicted tags from the result dictionary\n",
    "    final_predict=result_dict[file_path][\"generated_tags\"]\n",
    "    # Sort the predicted and ground truth data into categories using json_sorting function\n",
    "    category_dict=json_sorting(final_predict,groundtruth)\n",
    "    # Get the evaluation score by comparing the predicted tags with the ground truth\n",
    "    evaluation_results=get_score(category_dict)\n",
    "    # Prepare a dictionary with image path, ground truth, predictions, and evaluation results\n",
    "    data = {\n",
    "            'image_path': file_path,\n",
    "            'groundtruth': groundtruth,\n",
    "            'final_predictor': predicted_dict,\n",
    "            'evaluation_score': evaluation_results[\"avg_score\"],\n",
    "            'sum_corrected_values': evaluation_results[\"sum_corrected_values\"],\n",
    "            'sum_predicted_values': evaluation_results[\"sum_predicted_values\"]\n",
    "            }\n",
    "    # Append the data dictionary to the result_list\n",
    "    result_list.append(data)\n",
    "    # Pause execution for 30 seconds to avoid rate limiting\n",
    "    time.sleep(30)\n",
    "\n",
    "# data_df = spark.createDataFrame([data], schema=schema)\n",
    "# data_df.write.format(\"delta\").mode(\"append\").saveAsTable(evaluation_table)\n",
    "# ##################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e77372d-723d-4374-9131-c275e7809be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the CSV file name\n",
    "csv_file = \"Evaluation_without_agent_final.csv\"\n",
    "\n",
    "# Get the keys from the first dictionary to use as column headers\n",
    "keys = result_list[0].keys()\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=keys)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    writer.writerows(result_list)\n",
    "\n",
    "print(f\"Data has been written to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd6f430-038c-4b22-9f05-806464440746",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Agentic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e98f3f0-baf3-48aa-9a17-0efd320d797b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm_config_azure = [\n",
    "    {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": base_url,\n",
    "        \"api_type\": api_type,\n",
    "        \"api_version\": api_version,\n",
    "        \"temperature\":0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\"config_list\": llm_config_azure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ce0b0f-d915-4445-b0ea-5bdafc2ed08e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# User proxy\n",
    "tip_message = \"\\n If you do your BEST WORK, I'll tip you $100!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ea7556-8fc7-409c-9ebe-104bbf1ba827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### User Proxy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35ba1ce-cdbb-4256-88ce-f96b8205d5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Once the task is completed, answer 'TERMINATE-AGENT'\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"TERMINATE-AGENT\" in msg[\"content\"].lower(),\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fca49f-004f-4831-88c3-40ed4bd10d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Predictor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5979430-f099-4528-9591-39f7fa64ebc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_json = {\"generated_tags\": [\n",
    "                                {\n",
    "                                \"products category\": \"string\",\n",
    "                                \"Products\": \"string\",\n",
    "                                \"Color\": \"string\",\n",
    "                                \"Gender\": \"string\",\n",
    "                                \"Pattern\": \"string\",\n",
    "                                \"Silhouette\": \"string\",\n",
    "                                \"Neckline\": \"string\",\n",
    "                                \"Sleeve Length\": \"string\",\n",
    "                                \"Sleeve Style\": \"string\",\n",
    "                                \"Fabric\": \"string\",\n",
    "                                \"Brand\": \"string\",\n",
    "                                \"Occasion\": \"string\",\n",
    "                                \"Fit Type\": \"string\",\n",
    "                                \"Top Wear Length\": \"string\",\n",
    "                                }\n",
    "                                ]\n",
    "                }\n",
    "\n",
    "predictor_system_message = f\"\"\"\n",
    "You will receive an image_path as input.\n",
    "1. Analyze the provided catalog image and identify all the topwear and/or bottomwear items that are in focus and fully captured within the image.\n",
    "2. For each identified clothing item, extract the following metadata:\n",
    "\t{tags}\n",
    "3. Organize the metadata for each identified clothing item in a structured JSON format, with each item represented as a dictionary containing the specified keys.\n",
    "4. If the color recognition tags provided do not cover all the colors present in the image, feel free to use additional color descriptors as necessary.\n",
    "5. Ensure that the gender and material composition type are correctly mapped based on the identified products.\n",
    "6. Carefully match the provided tags with the appropriate metadata entities, and do not include any additional comments in the JSON output.\n",
    "7. If the image contains clothing items that are not fully captured or are out of focus, do not include them in the output.\n",
    "8. If the image does not contain any topwear or bottomwear items, or if the provided tags do not match the contents of the image, respond with an empty JSON array.\n",
    "\n",
    "\n",
    "<thinking>\n",
    "Review the provided image and tags, and extract the relevant metadata for each identified clothing item according to the instructions above. Organize the metadata in a structured JSON format, ensuring that all required fields are populated accurately.\n",
    "</thinking>\n",
    "\n",
    "Answer in the below format and DO NOT explain anything else:\n",
    "\n",
    "Format:\n",
    " {str(metadata_json)}\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "predictor = MultimodalConversableAgent(\n",
    "    name=\"predictor\", system_message=predictor_system_message, llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49009ba1-f1ff-4aa3-a2be-c3e33533b559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Evaluator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8208010-1b38-41a7-8b91-cac6fa5fbbd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_response_evaluator = {\n",
    "    \"generated_tags_with_critic\": [\n",
    "        [{\"tag\": \"Products\", \"value\": \"T-shirt\", \"score\": 1, \"critic_message\": \"\"},\n",
    "        {\"tag\": \"Color\", \"value\": \"Blue\", \"score\": 1, \"critic_message\": \"\"},\n",
    "        {\"tag\": \"Pattern\",\"value\": \"Solid\",\"score\": 0,\"critic_message\": \"The pattern is actually striped.\"}]\n",
    "    ],\n",
    "}\n",
    "\n",
    "evaluator = MultimodalConversableAgent(\n",
    "    name=\"evaluator\",\n",
    "    system_message=f\"\"\"You are an evaluator agent. Your task is to evaluate the accuracy of the tags generated for a given image.\n",
    "    You will receive an image and generated tags as inputs.\n",
    "    Analyze the image thoroughly and for each of the generated tag, give a score of 0 or 1. 0 if incorrect, 1 if it is correct.\n",
    "    If the score is 0 for a given tag, also give a critic message for that particular tag.\n",
    "    Finally answer the image and your generated tags with score and message. Follow the example response given below\n",
    "    \n",
    "    Refer to the pre-defined tags and values. Your evaluation must be based on this.\n",
    "    pre-defined tags: {tags}\n",
    "\n",
    "    If your evaluation score for all the tags are 1 and all tags are correct, then must write 'ALL-GOOD' at last.\n",
    "    Sample response format: \n",
    "    {sample_response_evaluator}\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d4425c-37b4-47ff-b9d8-5cccc0fb7dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Terminator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501db2aa-90e4-4f95-9e9b-87a982d35b64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "terminator_system_message = f\"\"\"\n",
    "\n",
    "You need to answer with 'max-3-tries'. Do NOT add any introductory phrase or do NOT explain anything else.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tip_message = \"\\nIf you do your BEST WORK, I'll tip you $100!\"\n",
    "\n",
    "terminator = autogen.AssistantAgent(\n",
    "    name=\"terminator\",\n",
    "    system_message=terminator_system_message + tip_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ae9b89-bc2a-47e4-9b27-25a7fe893c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_name_occurrences(data, name_value, no_of_iters):\n",
    "    count = sum(1 for entry in data if entry.get(\"name\") == name_value)\n",
    "    # print('count: ', count)\n",
    "    return count >= no_of_iters\n",
    "\n",
    "\n",
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    # print('messages: ', messages)\n",
    "    # print('groupchat_messages', messages)\n",
    "    if last_speaker is user_proxy:\n",
    "        # init -> retrieve\n",
    "        return predictor\n",
    "    if last_speaker is predictor:\n",
    "        return evaluator\n",
    "\n",
    "    if last_speaker is evaluator:\n",
    "        if \"all-good\" in messages[-1][\"content\"].lower():\n",
    "            None\n",
    "        elif check_name_occurrences(messages, \"evaluator\", 3):\n",
    "            return terminator\n",
    "        else:\n",
    "            return predictor\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, predictor, evaluator, terminator],\n",
    "    messages=[],\n",
    "    max_round=50,\n",
    "    speaker_selection_method=state_transition,\n",
    ")\n",
    "\n",
    "\n",
    "manager_system_message = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, system_message=manager_system_message, llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa058310-f729-45bf-9bb9-ccbb074d378b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Delete cache folder\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98a0191-8f2f-4051-8cf3-023bcda8b379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_Cach_folder():\n",
    "      \n",
    "    # Define the directory name\n",
    "    dir_name = \".cache\"\n",
    "    \n",
    "    # # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # # Construct the full path to the directory\n",
    "    dir_path = os.path.join(current_dir, dir_name)\n",
    "    \n",
    "    # # Check if the directory exists\n",
    "    if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
    "        # Remove the directory and its contents\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"The directory '{dir_name}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"The directory '{dir_name}' does not exist.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9beab89-88f9-4132-8cee-dbf95158eb11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get final dictionary \n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf2e476-dde5-4a00-8c89-402132083168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_final_dict(result, model_type):\n",
    "    try:\n",
    "        predict_dict={}\n",
    "        for num,pr in enumerate([history for history in result.chat_history if history['name']==\"predictor\"]):\n",
    "            # predictor_results=pr['content'][pr['content'].find(\"[\"):pr['content'].rfind(\"]\")+1].replace(\"```json\",\"\").replace(\"```\",\"\")\n",
    "            # predict_dict[num]=ast.literal_eval(predictor_results)\n",
    "            predict_dict[num]=eval(pr['content'].replace(\"```json\",\"\").replace(\"```\",\"\"))\n",
    "\n",
    "        return str(predict_dict[max(predict_dict.keys())])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e65cc5-f928-4d34-aa17-de410d78a877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_predict=eval(result_dict[\"Semi-Formal/Men/img2.jpg\"])[\"generated_tags\"]\n",
    "# final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f766f2ad-7548-4199-9a20-1bfa56f67130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# def merge_dicts_in_order(dict1, dict2):\n",
    "#             # Start with dict1 to preserve its order\n",
    "#             merged_dict = OrderedDict(dict1)\n",
    "#             for key, value2 in dict2.items():\n",
    "#                 value1 = dict1.get(key, \"\")\n",
    "#                 if key in merged_dict:\n",
    "#                     # If both values are non-empty and not the same, concatenate them\n",
    "#                     if value1 and value2 and value1 != value2:\n",
    "#                         merged_dict[key] = f\"{value1}, {value2}\"\n",
    "#                     else:\n",
    "#                         # Otherwise, keep the non-empty value or an empty string if both are empty\n",
    "#                         merged_dict[key] = value1 or value2\n",
    "#                 else:\n",
    "#                     # Add keys from dict2 that are not in dict1\n",
    "#                     merged_dict[key] = value2\n",
    "                    \n",
    "#             return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068ea44e-8112-4f0c-8905-3a7705d50943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\" Total Images ::\",len(final_filter_product_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9b9b9c-5acf-46f6-adfc-39d0c4e42ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_dict={}\n",
    "result_list=[]\n",
    "\n",
    "for file_path in final_filter_product_list:\n",
    "    print(file_path)\n",
    "    #--------------------- Load and display the image----------------------------\n",
    "    image_path=\"/dbfs/mnt/my_intellitag_mount/Intellitag_Testing/\" + file_path\n",
    "\n",
    "    top_wear_filter_list=top_wear_df.loc[top_wear_df['KEY']==file_path].to_dict(orient='records')\n",
    "    bottom_wear_filter_list=bottom_wear_df.loc[bottom_wear_df['KEY']==file_path].to_dict(orient='records')\n",
    "\n",
    "    final_ground_truth_list=top_wear_filter_list+bottom_wear_filter_list\n",
    "     \n",
    "    #-----------------------Run the Agent ---------------------------------------\n",
    "    img_path_format = f\"image_path:   <img {image_path}>\"\n",
    "    result = user_proxy.initiate_chat(\n",
    "        manager, message=img_path_format, summary_method=\"reflection_with_llm\"\n",
    "    )\n",
    "\n",
    "    #----------------------Get final dict --------------------------\n",
    "    final_dict=get_final_dict(result,\"gpt-4o\")\n",
    "    result_dict[file_path]=final_dict\n",
    "    #################\n",
    "    groundtruth=final_ground_truth_list\n",
    "    final_predict=eval(result_dict[file_path])[\"generated_tags\"]\n",
    "    final_predict\n",
    "    category_dict=json_sorting(final_predict,groundtruth)\n",
    "    evaluation_results=get_score(category_dict)\n",
    "    data = {\n",
    "        'image_path': file_path,\n",
    "        'groundtruth': final_ground_truth_list,\n",
    "        'final_predictor': final_dict,\n",
    "        'evaluation_score': evaluation_results[\"avg_score\"],\n",
    "        'sum_corrected_values': evaluation_results[\"sum_corrected_values\"],\n",
    "        'sum_predicted_values': evaluation_results[\"sum_predicted_values\"]\n",
    "        }\n",
    "    # return embed_data\n",
    "    result_list.append(data)\n",
    "    # delete cache folder\n",
    "    delete_Cach_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ce9142-6032-4e82-afad-352027b497bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the CSV file name\n",
    "csv_file = \"Evaluation_agentic_final.csv\"\n",
    "\n",
    "# Get the keys from the first dictionary to use as column headers\n",
    "keys = result_list[0].keys()\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=keys)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    writer.writerows(result_list)\n",
    "\n",
    "print(f\"Data has been written to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e4a121-5fa9-42fb-bc20-0e9c5d8d5a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Evaluation_without_and_with_agent",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
